# Dataset-Statistics

This library has been built to ease downloading and processing datasets from the S3 Siltbucket, Satsim, and SatNet. It also create statistics and can apply corrections, create COCO formatted datasets, filter datasets, 

## Usage

Primarily, users will use eihter the "data_download_tool.py" to download files, "annotation_viewer.py" to generate PNGs of annotations and full images, and "plot.ipynb" to generate plots about statistics in the whole dataset. 

### Connecting to Data Repositories

First, you must be granted access to the AWS silt S3 bucket, this contains all the annotations generated by enabled intelligence. In order to use this, I highly reccomend downloading the AWS extension on VS Code. Download the extension, then, login to the AWS ai_autonomy gov cloud and find the area labled "access keys". On the AWS extension "explorer", click the three buttons on the top right where it says "Connect to AWS". From here create a profile for your AWS connection. Then click Edit credentials, and it should lead you to a credentials page. From the AWS AI_Autonomy gov cloud, copy over the following credentials file

```
[default]
aws_access_key_id=...
aws_secret_access_key=...
aws_session_token=...
```

and the following into the config file
```
[default]
region = us-gov-west-1
```

If you plan on downloading data from the UDL, you must create an account on the UDL and find the correct download key. To do this, once you have an account and go to the store front -> UDL API-> Sky Imagery -> GET /udl/skyimagery/getFile/{id} -> Try it out -> Enter in ID 00012407-bcd5-4188-9371-0926261a6065 -> execute. From the resulting response you will find 

```
curl -X 'GET' \
  'https://unifieddatalibrary.com/udl/skyimagery/getFile/00012407-bcd5-4188-9371-0926261a6065' \
  -H 'accept: application/octet-stream' \
  -H 'Authorization: Basic [UDL Key]'
```

Copy the [UDL Key] into a file called /src/UDL_KEY.py under the variable name UDL_KEY.

### Downloading Data
Make sure to connect the repositories in the previous step to download data. If you are downloading data, find which silt directory you intend to download from using the AWS extension. (This is known not to work on ADA4, so try locally or phone a friend for a directory). Under the python notebook named "DatabaseDownloader.ipynb", there are examples of how to download data. The S3Client object requires a SILT directory, and scans and pairs all of the annotations with fits files. IF no fits files are found on SILT, then it will wait to download them from UDL. 

```python
downloader = S3Client(RME04_PATH) #Initializes downloader on desired database path. Gives list of dates to download
download_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME04-2025_Annotations"

#Dates displayed by the initialization of the S3 Client
RME04_dates = ["2025-05-09","2025-05-12","2025-05-14","2025-05-15","2025-05-16","2025-05-19","2025-05-21","2025-05-22","2025-05-23","2025-05-27","2025-05-28","2025-05-30","2025-06-02","2025-06-03","2025-06-04","2025-06-05","2025-06-06","2025-06-09","2025-06-10","2025-06-11","2025-06-13","2025-06-16","2025-06-17","2025-06-18","2025-06-24","2025-06-25","2025-06-26","2025-07-01","2025-07-02","2025-07-03","2025-07-07","2025-07-08","2025-07-09","2025-07-10","2025-07-11","2025-07-14","2025-07-15","2025-07-16","2025-07-17","2025-07-18","2025-07-21","2025-07-22","2025-07-23","2025-07-24","2025-07-25","2025-07-28","2025-07-29","2025-07-30","2025-07-31","2025-08-01","2025-08-04","2025-08-05","2025-08-06","2025-08-07","2025-08-08","2025-08-11","2025-08-12","2025-08-13","2025-08-14","2025-08-15","2025-08-18","2025-08-19","2025-08-20","2025-08-21","2025-08-22","2025-08-25","2025-08-26","2025-08-27","2025-08-28","2025-08-29","2025-09-02","2025-09-03","2025-09-04","2025-09-05","2025-09-08","2025-09-09","2025-09-10","2025-09-11","2025-09-12","2025-09-15","2025-09-16","2025-09-17","2025-09-18","2025-09-19","2025-09-22","2025-09-23","2025-09-24","2025-09-25","2025-09-26","2025-09-29","2025-09-30","2025-10-01","2025-10-02","2025-10-03","2025-10-06"]

#Iterating through each date and downloading data to the desired download_directory
for date in RME04_dates:
    downloader.download_annotation_dates(date, download_directory)
```

There are other download tools but I reccomend the download_annotation_dates for ease of use. Downloading data creates raw_datasets. I highly reccomend creating download scripts for repeatability instead of python notebooks. 

### Raw Datasets

Raw datasets contain the raw annotation information downloaded from the UDL. To initialize a raw dataset follow the following script 

```python
from src.raw_dataset import raw_dataset

path = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025_Annotations/2025-04-25"
dataset = raw_dataset("/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025_Annotations/2025-04-25")

```
which will cause the raw dataset object to calculate statistics on all attributes and annotations of the raw data. Raw datasets can also be modified with annotation corrections, or examined for quality assurance or create a hand selected dataset. 

#### Bounding Box Corrections

This python notebook applies bounding box correction to all annotations in a raw dataset. You can use a script or use the python notebook. If you set "require_approval" to true, you can manually accept or reject a particular dataset correction. This only recentroids the bounding boxes and does not resize the box. 

```Python
# %matplotlib widget
from src.raw_datset import raw_dataset

path="/mnt/c/Users/david.chaparro/Documents/Repos/Dataset_Statistics/data/RME04Sat-2025-05-10_last_take"
local = raw_dataset(path)
local.correct_annotations(require_approval=False)
```

#### Target Injection



#### Error Characterizer

To quantify the quality of an EI dataset, use the file called "Error_Characterizer.ipynb". The first code block launches a error characterizer. Simply view the image and enter the number of the type of error as listed on the python notebook. You can stop at any time and your labels will be kept. Once the characterizer is stopped, you can move to the second code block to print an error report of the dataset. To use this, enter the directory of the raw dataset you want to examine

```python
from src.raw_datset import raw_dataset, StatisticsFile
import os
from src.plots import plot_categorical_column, plot_stacked_errors_with_percent_legend, plot_stacked_errors_with_percent_legend_by_annotator_id

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025Data"
```

#### Create Hand Selected Datasets
If you want to create a hand selected dataset for evaluation, you can use the python notebooks called "SelectNewDataset.ipynb" or "SelectNewDatasetByCollect.ipynb". These notebooks create a UI to select individual images or collects into a new directory. Enter a new_directory to create the new dataset and an old directory to copy from. When prompted, if no character is entered, then it will leave the image, and if any character is input, that image will be copied to the new directory. There is also multiple "move_mode" options, such as "copy" and "move" which either copies or moves the file respectively. "move" deletes it from the old dataset permanently. 

```python
from src.raw_datset import raw_dataset, StatisticsFile

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025Data"
new_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/Test_dataset"
local_files = raw_dataset(directory)
local_files.create_hand_selected_dataset(new_directory)
# or
local_files.create_hand_selected_dataset_by_collect(new_directory)
```

#### Label Star and Target Quality

If you want to label star quality and target quality similar to Alex's MDP dataset, you can use the "CreateStar-TargetQualityDataset.ipynb". This copies or moves images labeled with a star quality and target quality. Both numbers must be entered in order for the annotation to count. Using move_mode, you can remove or keep the files in the original dataset.

```python
from src.raw_datset import raw_dataset

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025Data"
new_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/Test_dataset"
local_files = raw_dataset(directory)
local_files.create_target_quality_dataset(new_directory)
```

#### Create Calsat Dataset

### Coco Datasets
