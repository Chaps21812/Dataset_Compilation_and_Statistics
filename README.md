# Dataset-Statistics

This library has been built to ease downloading and processing datasets from the S3 Siltbucket, Satsim, and SatNet. It also create statistics and can apply corrections, create COCO formatted datasets, filter datasets, 

Primarily, users will use eihter the "data_download_tool.py" to download files, "annotation_viewer.py" to generate PNGs of annotations and full images, and "plot.ipynb" to generate plots about statistics in the whole dataset. 

## Connecting to Data Repositories

First, you must be granted access to the AWS silt S3 bucket, this contains all the annotations generated by enabled intelligence. In order to use this, I highly reccomend downloading the AWS extension on VS Code. Download the extension, then, login to the AWS ai_autonomy gov cloud and find the area labled "access keys". On the AWS extension "explorer", click the three buttons on the top right where it says "Connect to AWS". From here create a profile for your AWS connection. Then click Edit credentials, and it should lead you to a credentials page. From the AWS AI_Autonomy gov cloud, copy over the following credentials file

```
[default]
aws_access_key_id=...
aws_secret_access_key=...
aws_session_token=...
```

and the following into the config file
```
[default]
region = us-gov-west-1
```

If you plan on downloading data from the UDL, you must create an account on the UDL and find the correct download key. To do this, once you have an account and go to the store front -> UDL API-> Sky Imagery -> GET /udl/skyimagery/getFile/{id} -> Try it out -> Enter in ID 00012407-bcd5-4188-9371-0926261a6065 -> execute. From the resulting response you will find 

```
curl -X 'GET' \
  'https://unifieddatalibrary.com/udl/skyimagery/getFile/00012407-bcd5-4188-9371-0926261a6065' \
  -H 'accept: application/octet-stream' \
  -H 'Authorization: Basic [UDL Key]'
```

Copy the [UDL Key] into a file called /src/UDL_KEY.py under the variable name UDL_KEY.

## Downloading Data
Make sure to connect the repositories in the previous step to download data. If you are downloading data, find which silt directory you intend to download from using the AWS extension. (This is known not to work on ADA4, so try locally or phone a friend for a directory). Under the python notebook named "DatabaseDownloader.ipynb", there are examples of how to download data. The S3Client object requires a SILT directory, and scans and pairs all of the annotations with fits files. IF no fits files are found on SILT, then it will wait to download them from UDL. 

```python
downloader = S3Client(RME04_PATH) #Initializes downloader on desired database path. Gives list of dates to download
download_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME04-2025_Annotations"

#Dates displayed by the initialization of the S3 Client
RME04_dates = ["2025-05-09","2025-05-12","2025-05-14","2025-05-15","2025-05-16","2025-05-19","2025-05-21","2025-05-22","2025-05-23","2025-05-27","2025-05-28","2025-05-30","2025-06-02","2025-06-03","2025-06-04","2025-06-05","2025-06-06","2025-06-09","2025-06-10","2025-06-11","2025-06-13","2025-06-16","2025-06-17","2025-06-18","2025-06-24","2025-06-25","2025-06-26","2025-07-01","2025-07-02","2025-07-03","2025-07-07","2025-07-08","2025-07-09","2025-07-10","2025-07-11","2025-07-14","2025-07-15","2025-07-16","2025-07-17","2025-07-18","2025-07-21","2025-07-22","2025-07-23","2025-07-24","2025-07-25","2025-07-28","2025-07-29","2025-07-30","2025-07-31","2025-08-01","2025-08-04","2025-08-05","2025-08-06","2025-08-07","2025-08-08","2025-08-11","2025-08-12","2025-08-13","2025-08-14","2025-08-15","2025-08-18","2025-08-19","2025-08-20","2025-08-21","2025-08-22","2025-08-25","2025-08-26","2025-08-27","2025-08-28","2025-08-29","2025-09-02","2025-09-03","2025-09-04","2025-09-05","2025-09-08","2025-09-09","2025-09-10","2025-09-11","2025-09-12","2025-09-15","2025-09-16","2025-09-17","2025-09-18","2025-09-19","2025-09-22","2025-09-23","2025-09-24","2025-09-25","2025-09-26","2025-09-29","2025-09-30","2025-10-01","2025-10-02","2025-10-03","2025-10-06"]

#Iterating through each date and downloading data to the desired download_directory
for date in RME04_dates:
    downloader.download_annotation_dates(date, download_directory)
```

There are other download tools but I reccomend the download_annotation_dates for ease of use. Downloading data creates raw_datasets. I highly reccomend creating download scripts for repeatability instead of python notebooks. 

## Raw Datasets

Raw datasets contain the raw annotation information downloaded from the UDL. To initialize a raw dataset follow the following script 

```python
from src.raw_dataset import raw_dataset

path = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025_Annotations/2025-04-25"
dataset = raw_dataset("/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025_Annotations/2025-04-25")

```
which will cause the raw dataset object to calculate statistics on all attributes and annotations of the raw data. Raw datasets can also be modified with annotation corrections, or examined for quality assurance or create a hand selected dataset. 

### Bounding Box Corrections

This python notebook applies bounding box correction to all annotations in a raw dataset. You can use a script or use the python notebook. If you set "require_approval" to true, you can manually accept or reject a particular dataset correction. This only recentroids the bounding boxes and does not resize the box. 

```Python
# %matplotlib widget
from src.raw_datset import raw_dataset

path="/mnt/c/Users/david.chaparro/Documents/Repos/Dataset_Statistics/data/RME04Sat-2025-05-10_last_take"
local = raw_dataset(path)
local.correct_annotations(require_approval=False)
```

### Target Injection

`inject_targets_from_numpy` takes a raw dataset and injects targets prepared in a numpy array and injects them. Ask our Kevin Phan about how to format numpy arrays. Given a directory of saved numpy arrays, this function injects the targets into any raw file. 

### Error Characterizer

To quantify the quality of an EI dataset, use the file called "Error_Characterizer.ipynb". The first code block launches a error characterizer. Simply view the image and enter the number of the type of error as listed on the python notebook. You can stop at any time and your labels will be kept. Once the characterizer is stopped, you can move to the second code block to print an error report of the dataset. To use this, enter the directory of the raw dataset you want to examine

```python
from src.raw_datset import raw_dataset, StatisticsFile
import os
from src.plots import plot_categorical_column, plot_stacked_errors_with_percent_legend, plot_stacked_errors_with_percent_legend_by_annotator_id

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025Data"
```

### Create Hand Selected Datasets
If you want to create a hand selected dataset for evaluation, you can use the python notebooks called "SelectNewDataset.ipynb" or "SelectNewDatasetByCollect.ipynb". These notebooks create a UI to select individual images or collects into a new directory. Enter a new_directory to create the new dataset and an old directory to copy from. When prompted, if no character is entered, then it will leave the image, and if any character is input, that image will be copied to the new directory. There is also multiple "move_mode" options, such as "copy" and "move" which either copies or moves the file respectively. "move" deletes it from the old dataset permanently. 

```python
from src.raw_datset import raw_dataset, StatisticsFile

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025Data"
new_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/Test_dataset"
local_files = raw_dataset(directory)
local_files.create_hand_selected_dataset(new_directory)
# or
local_files.create_hand_selected_dataset_by_collect(new_directory)
```

### Label Star and Target Quality

If you want to label star quality and target quality similar to Alex's MDP dataset, you can use the "CreateStar-TargetQualityDataset.ipynb". This copies or moves images labeled with a star quality and target quality. Both numbers must be entered in order for the annotation to count. Using move_mode, you can remove or keep the files in the original dataset.

```python
from src.raw_datset import raw_dataset

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025Data"
new_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/Test_dataset"
local_files = raw_dataset(directory)
local_files.create_target_quality_dataset(new_directory)
```

### Create Calsat Dataset

You can create a dataset of calsats only by using the following code. The code can be found in the "CreateCalsatDataset.ipynb" file. 

```python
from src.raw_datset import raw_dataset

directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025_Annotations/2025-04-08"
new_directory = "/data/Dataset_Compilation_and_Statistics/Sentinel_Datasets/RME01-2025_Annotations/Calsattest"
dataset = raw_dataset(directory)
dataset.create_calsat_dataset(new_directory, "copy")
```

## Coco Datasets

Once a raw dataset has been processed and corrected, you can create coco datasets with various attributes, including a train test split, sample limited datasets, and sorting datasetsby attributes. Coco datasets are the most moldable form of datasets. For multiframe annotations, the folder "multiframe_annotations" has a folder which monitors and tracks the multiframe annotations. 

### silt_to_coco

`silt_to_coco` takes a raw dataset and converts it into a a coco dataset. Can create datasets of stars, satellites, or neither. Converts each fits file into a preprocessed PNG used during training. The preprocess function can be specified to create the desired preprocess function. 

```python
silt_dataset_path (list[str]): Input list of files to shuffle and generate a train test split
include_sats (bool): Include satellites in coco dataset
include_stars (bool): Include stars in coco dataset
zip (bool): Zip files into a compressed folder
convert_png (bool): Convert to an intermediate PNG  
process_func (src.preprocess_functions): Preprocess function to convert PNGs to
notes (str)
```


### merge_coco

`merge_coco` Converts a list of processed coco directories and merges them together, and optionally can creates a train test split with a specified train test split ratio

```python 
coco_directories (list[str]): List of paths of coco datasets. Raw datasets must be converted to coco before merging
destination_path (str): Destination of the merged dataset
notes (str): Notes to add onto the dataset annotation file for future use
train_test_split (bool): Bool to create a train test split or not
train_ratio (float): Ratio of training samples
val_ratio (float): Ratio of validation samples
test_ratio (float): Ratio of testing samples
zip (bool): Number of partitions to divide your data into
```

### partition_dataset

`partition_dataset` Can partition a dataset into multiple coco datasets sorted by a particular attribtue (SNR etc.). You can also use this to limit datasets to a desired size

```python 
coco_directories (list[str]): Coco dataset to partition or reduce
destination_paths (list[str]): Directories to split this coco dataset into
partition_attribute (str): String attribute of the image specific attribute to sort by
dataset_size (int): Limit to the size of the dataset
notes (str): Notes to add onto the dataset annotation file for future use
```