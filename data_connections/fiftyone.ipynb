{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "01f8b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fiftyone as fo\n",
    "from fiftyone.core.labels import Detections, Detection\n",
    "\n",
    "# fo.delete_dataset(\"my-coco-dataset\")\n",
    "dataset_directory = \"/home/davidchaparro/Datasets/MultiChannelLMNT02_Train/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24c32a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset name 'custom_coco_with_attributes' is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_2498760/1752193228.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Optional: load category names\u001b[39;00m\n\u001b[32m     18\u001b[39m category_map = {cat[\u001b[33m\"id\"\u001b[39m]: cat[\u001b[33m\"name\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;28;01min\u001b[39;00m coco.get(\u001b[33m\"categories\"\u001b[39m, [])}\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ---- CREATE DATASET ----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m dataset = fo.Dataset(name=dataset_name)\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ---- PROCESS SAMPLES ----\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_id, img \u001b[38;5;28;01min\u001b[39;00m image_map.items():\n",
      "\u001b[32m~/miniconda3/envs/DatasetStatistics/lib/python3.12/site-packages/fiftyone/core/singletons.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, name, _create, *args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m instance.deleted\n\u001b[32m     34\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m instance.name \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     35\u001b[39m         ):\n\u001b[32m     36\u001b[39m             instance = cls.__new__(cls)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m             instance.__init__(name=name, _create=_create, *args, **kwargs)\n\u001b[32m     38\u001b[39m             name = instance.name  \u001b[38;5;66;03m# `__init__` may have changed `name`\u001b[39;00m\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/DatasetStatistics/lib/python3.12/site-packages/fiftyone/core/dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, persistent, overwrite, _create, _virtual, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;28;01mand\u001b[39;00m dataset_exists(name):\n\u001b[32m    323\u001b[39m             delete_dataset(name)\n\u001b[32m    324\u001b[39m \n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _create:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m             doc, sample_doc_cls, frame_doc_cls = _create_dataset(\n\u001b[32m    327\u001b[39m                 self, name, persistent=persistent, **kwargs\n\u001b[32m    328\u001b[39m             )\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/DatasetStatistics/lib/python3.12/site-packages/fiftyone/core/dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj, name, persistent, _patches, _frames, _clips, _src_collection)\u001b[39m\n\u001b[32m   8343\u001b[39m     _frames=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   8344\u001b[39m     _clips=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   8345\u001b[39m     _src_collection=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   8346\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m8347\u001b[39m     slug = _validate_dataset_name(name)\n\u001b[32m   8348\u001b[39m \n\u001b[32m   8349\u001b[39m     _id = ObjectId()\n\u001b[32m   8350\u001b[39m     now = datetime.utcnow()\n",
      "\u001b[32m~/miniconda3/envs/DatasetStatistics/lib/python3.12/site-packages/fiftyone/core/dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(name, skip)\u001b[39m\n\u001b[32m    131\u001b[39m     )\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clashing_name_doc \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    133\u001b[39m         clashing_name = clashing_name_doc[\u001b[33m\"name\"\u001b[39m]\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m clashing_name == name:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"Dataset name '{name}' is not available\")\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    137\u001b[39m             raise ValueError(\n\u001b[32m    138\u001b[39m                 f\"Dataset name '{name}' is not available: slug '{slug}' \"\n",
      "\u001b[31mValueError\u001b[39m: Dataset name 'custom_coco_with_attributes' is not available"
     ]
    }
   ],
   "source": [
    "\n",
    "fo.delete_dataset(\"custom_coco_with_attributes\")\n",
    "# ---- CONFIG ----\n",
    "data_path = os.path.join(dataset_directory)  # directory with image files\n",
    "labels_path = os.path.join(dataset_directory, \"annotations\", \"annotations.json\")\n",
    "dataset_name = \"custom_coco_with_attributes\"\n",
    "\n",
    "# ---- LOAD COCO JSON ----\n",
    "with open(labels_path) as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Build ID lookups\n",
    "image_map = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "annotations_map = {}\n",
    "for ann in coco[\"annotations\"]:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    annotations_map.setdefault(img_id, []).append(ann)\n",
    "\n",
    "# Optional: load category names\n",
    "category_map = {cat[\"id\"]: cat[\"name\"] for cat in coco.get(\"categories\", [])}\n",
    "\n",
    "# ---- CREATE DATASET ----\n",
    "dataset = fo.Dataset(name=dataset_name)\n",
    "\n",
    "# ---- PROCESS SAMPLES ----\n",
    "for img_id, img in image_map.items():\n",
    "    filepath = os.path.join(data_path, img[\"file_name\"])\n",
    "    sample = fo.Sample(filepath=filepath)\n",
    "\n",
    "    # Add all image-level fields (skip COCO-specific ones like id/file_name if undesired)\n",
    "    for key, value in img.items():\n",
    "        if key not in {\"id\", \"file_name\"}:\n",
    "            sample[key] = value\n",
    "\n",
    "    # Add detection annotations\n",
    "    detections = []\n",
    "    for ann in annotations_map.get(img_id, []):\n",
    "        label = category_map.get(ann[\"category_id\"], str(ann[\"category_id\"]))\n",
    "        bbox = ann[\"bbox\"]  # COCO format: [x, y, width, height]\n",
    "        # Convert COCO bbox to [x, y, w, h] normalized\n",
    "        width = img[\"width\"]\n",
    "        height = img[\"height\"]\n",
    "        rel_bbox = [\n",
    "            bbox[0] / width,\n",
    "            bbox[1] / height,\n",
    "            bbox[2] / width,\n",
    "            bbox[3] / height,\n",
    "        ]\n",
    "\n",
    "        det = Detection(label=label, bounding_box=rel_bbox)\n",
    "\n",
    "        # Add all other annotation attributes\n",
    "        for key, value in ann.items():\n",
    "            if key not in {\"id\", \"image_id\", \"bbox\", \"category_id\", \"iscrowd\"}:\n",
    "                det[key] = value\n",
    "\n",
    "        # Optional: copy nested 'attributes' dictionary if present\n",
    "        if \"attributes\" in ann and isinstance(ann[\"attributes\"], dict):\n",
    "            for attr_key, attr_value in ann[\"attributes\"].items():\n",
    "                det[attr_key] = attr_value\n",
    "\n",
    "        detections.append(det)\n",
    "\n",
    "    # Add detection field to sample\n",
    "    if detections:\n",
    "        sample[\"ground_truth\"] = Detections(detections=detections)\n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "print(f\"Imported {len(dataset)} samples into '{dataset_name}'\")\n",
    "\n",
    "# ---- OPTIONAL: Launch UI ----\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825ab4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruah\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.coco as fouc\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# class CustomCOCODetectionImporter(COCODetectionDatasetImporter):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "\n",
    "#         # # Build a lookup from image ID to its custom metadata\n",
    "#         # self._image_metadata_map = {}\n",
    "#         # for image in self._images:\n",
    "#         #     self._image_metadata_map[image[\"id\"]] = image\n",
    "\n",
    "#     def _parse_ann(self, ann, image_metadata):\n",
    "#         # Call the base method to create the detection\n",
    "#         detection = super()._parse_ann(ann, image_metadata)\n",
    "\n",
    "#         # Fallback if attributes are flat in the annotation\n",
    "#         for k in [\"flux\",\"local_prominence\", \"local_snr\", \"max_signal_diff\", \"global_snr\"]:  # <-- customize your fields\n",
    "#             if k in ann:\n",
    "#                 detection[k] = ann[k]\n",
    "\n",
    "#         return detection\n",
    "\n",
    "#     def _parse_sample(self, image, anns, metadata):\n",
    "#         # This gets called per image\n",
    "#         detections = [self._parse_ann(a, metadata) for a in anns]\n",
    "#         detections = Detections(detections=detections)\n",
    "\n",
    "#         sample = Sample(filepath=os.path.join(self.data_path, image[\"file_name\"]))\n",
    "#         sample[self.label_field] = detections\n",
    "\n",
    "#         # Image-level attributes\n",
    "#         for k in [\"QA\", \"label_created\", \"label_updated\", \"num_objects\", \"exposure\", \"std_intensity\", \"median_intensity\", \"dates\", \"times\", \"streak_direction_std\", \"streak_direction_mean\", \"streak_length_mean\", \"streak_length_std\", \"num_stars\", \"num_sats\"]:  # <-- customize your fields\n",
    "#             if k in image:\n",
    "#                 sample[k] = image[k]\n",
    "\n",
    "#         return sample\n",
    "    \n",
    "\n",
    "# fo.delete_dataset(\"my-coco-dataset\")\n",
    "# dataset_directory = \"/home/davidchaparro/Datasets/MultiChannelLMNT02_Train/train\"\n",
    "# importer = CustomCOCODetectionImporter(\n",
    "#     data_path= os.path.join(dataset_directory),\n",
    "#     labels_path=os.path.join(dataset_directory, \"annotations\", \"annotations.json\"),\n",
    "#     include_id=True,\n",
    "#     extra_attrs=True\n",
    "# )\n",
    "# dataset = fo.Dataset(name=\"my-coco-dataset\")\n",
    "# dataset.add_importer(importer)\n",
    "\n",
    "# dataset_directory = \"/home/davidchaparro/Datasets/MultiChannelLMNT02_Train/train\"\n",
    "# dataset = fo.Dataset.from_dir(\n",
    "#     dataset_type=fo.types.COCODetectionDataset,\n",
    "#     data_path= os.path.join(dataset_directory),\n",
    "#     labels_path=os.path.join(dataset_directory, \"annotations\", \"annotations.json\")\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# dataset_directory = \"/home/davidchaparro/Datasets/MultiChannelLMNT02_Train/train\"\n",
    "# importer = CustomCOCODetectionImporter(\n",
    "#     data_path= os.path.join(dataset_directory),\n",
    "#     labels_path=os.path.join(dataset_directory, \"annotations\", \"annotations.json\"),\n",
    "#     include_id=True,\n",
    "#     extra_attrs=True\n",
    "# )\n",
    "# dataset = fo.Dataset(name=\"my-coco-dataset\")\n",
    "# dataset.add_importer(importer)\n",
    "\n",
=======
   "execution_count": null,
   "id": "87609014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
>>>>>>> 63d148838269195ce735fb3a183d93c1ad73fd4c
    "\n",
    "\n",
    "\n",
    "# Load a sample dataset\n",
<<<<<<< HEAD
    "# dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "# session = fo.launch_app(dataset)\n",
    "print(\"bruah\")\n",
=======
    "dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "session = fo.launch_app(dataset)\n",
    "session.show()\n",
>>>>>>> 63d148838269195ce735fb3a183d93c1ad73fd4c
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DatasetStatistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
<<<<<<< HEAD
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
=======
   "name": "python",
>>>>>>> 63d148838269195ce735fb3a183d93c1ad73fd4c
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
